{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulsumaltintas/Natural_Language_Processing/blob/main/natural_language_processing_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK (Natural Language Toolkit)--> kullanacağımız kütüphane\n",
        "# spaCy python üzerine kurulu.C de derlenmiş bir kütüphane\n",
        "#gensim başka bir kütüphane"
      ],
      "metadata": {
        "id": "qnR9QULwqdne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiGd5y79u33x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1576011-87a2-4e4d-b32a-22193d4ef97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download() #nltk nın görsel arayüzünü görmek için bu komutu çalıştırıyoruz."
      ],
      "metadata": {
        "id": "EtRoG2_Xqhrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization bir metni bizim kullanacağımız şekle dönüştürme.\n",
        "# Tokenization metnimizi işleyebilmek için anlamlı alt parçalara ayırma işlemidir.Daha alt parçalara bölünemeyecek şekilde olacak.\n",
        "# Metnin içerisindeki cümleleri çıkarmak istiyorsak buna sencetece tokenization,cümlelerin içerisindeki kelimeleri çıkarmak istiyorsak buna da word tokenization deniyor.\n",
        "# Token--> metnin içerisindeki en küçük parçalar."
      ],
      "metadata": {
        "id": "gBeS_9EIqlpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenize**"
      ],
      "metadata": {
        "id": "cD4piA7crcXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gCL6qTylQve",
        "outputId": "3d2690d7-e05f-4b5d-c14a-2cb549c61fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiS3VA1TvNgL"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "str = \"İstanbul Türkiye'nin kuzeybatısında, Marmara kıyısı ve Boğaziçi boyunca, Haliç'i de çevreleyecek şekilde kurulmuştur.\"\n",
        "str2 = \"I'm going to school.\" # I am in ayrı kelimeler olduğunu biliyor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hwtONz4vRtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e171d47-8bf5-46e9-f0ac-90335581b9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['İstanbul', \"Türkiye'nin\", 'kuzeybatısında', ',', 'Marmara', 'kıyısı', 've', 'Boğaziçi', 'boyunca', ',', 'Haliç', \"'\", 'i', 'de', 'çevreleyecek', 'şekilde', 'kurulmuştur', '.']\n",
            "['I', \"'m\", 'going', 'to', 'school', '.']\n"
          ]
        }
      ],
      "source": [
        "print(word_tokenize(str)) # bu işlem kesme işaretiyle ayrılan sözcükleri ayrı ayrılmıyor tek sözcük gibi alıyor. Bu kelimenin bir kelime olduğunu biliyor.\n",
        "print(word_tokenize(str2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gx0DUK8vZz3"
      },
      "outputs": [],
      "source": [
        "#Her zaman doğru sonuç verecek diye bir şey yok.\n",
        "#Bu yüzden dil bazlı düşünülmeli ve kendi tokenizerlarımızı oluşturmamız gerekir.Türkçe için ayrı bir tokenizer gibi.\n",
        "#Regular expression ile kendi tokenizerımızı yazabiliriz."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenize**"
      ],
      "metadata": {
        "id": "jOIt4i3lrf5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "9RCJPsoHrmNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str3 = \"İstanbul Türkiye'nin kuzeybatısında, Marmara kıyısı ve Boğaziçi boyunca, Haliç'i de çevreleyecek şekilde kurulmuştur. İstanbul kıtalararası bir şehir olup, Avrupa'daki bölümüne Avrupa Yakası veya Rumeli Yakası, Asya'daki bölümüne ise Anadolu Yakası veya Asya Yakası denir. Tarihte ilk olarak üç tarafı Marmara Denizi, Boğaziçi ve Haliç'in sardığı bir yarımada üzerinde kurulan İstanbul'un batıdaki sınırını İstanbul Surları oluşturmaktaydı. Gelişme ve büyüme sürecinde surların her seferinde daha batıya ilerletilerek inşa edilmesiyle dört defa genişletilen şehrin[12] 39 ilçesi vardır. Sınırları içerisinde ise büyükşehir belediyesi ile birlikte toplam 40 belediye bulunmaktadır.\""
      ],
      "metadata": {
        "id": "SUP0ZGkfrszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(str3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZOo4bbOsulG",
        "outputId": "c5917c30-5968-42b4-ba77-6dd0e56c127c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"İstanbul Türkiye'nin kuzeybatısında, Marmara kıyısı ve Boğaziçi boyunca, Haliç'i de çevreleyecek şekilde kurulmuştur.\", \"İstanbul kıtalararası bir şehir olup, Avrupa'daki bölümüne Avrupa Yakası veya Rumeli Yakası, Asya'daki bölümüne ise Anadolu Yakası veya Asya Yakası denir.\", \"Tarihte ilk olarak üç tarafı Marmara Denizi, Boğaziçi ve Haliç'in sardığı bir yarımada üzerinde kurulan İstanbul'un batıdaki sınırını İstanbul Surları oluşturmaktaydı.\", 'Gelişme ve büyüme sürecinde surların her seferinde daha batıya ilerletilerek inşa edilmesiyle dört defa genişletilen şehrin[12] 39 ilçesi vardır.', 'Sınırları içerisinde ise büyükşehir belediyesi ile birlikte toplam 40 belediye bulunmaktadır.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regexp Tokenizer**"
      ],
      "metadata": {
        "id": "waWQimLjs9Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "str4 = \"İstanbul Türkiye'nin kuzeybatısında, Marmara kıyısı ve Boğaziçi boyunca, Haliç'i de çevreleyecek şekilde kurulmuştur.\"\n",
        "\n",
        "tkn = RegexpTokenizer(\"\\w+\") #\\s ile aynı \\w--> alfa nümerik karakterlei alır.\n",
        "#\\w+--> bir veya daha fazla karakterleri alır(kelime kelime)\n",
        "#Boşlukları bize verecek.split() işlemi ile de bu işlem gerçekleştiriliyor.\n",
        "#split() ile kullanıldığında . da son kelime ile beraber alınır fakat biz noktalama işaretlerini de ayrı bir eleman olarak almak istiyoruz.\n",
        "#print(str4.split(' ')) ile de gerçekleştirilebirlir.Noktayı ayrı bir eleman olarak almadığı için bunu kullanmıyoruz.\n",
        "\n",
        "print(tkn.tokenize(str4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcnQna0UtEM-",
        "outputId": "1b6a6da8-d483-4ecf-a1c5-7c94253e9e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['İstanbul', 'Türkiye', 'nin', 'kuzeybatısında', 'Marmara', 'kıyısı', 've', 'Boğaziçi', 'boyunca', 'Haliç', 'i', 'de', 'çevreleyecek', 'şekilde', 'kurulmuştur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n-Grams**"
      ],
      "metadata": {
        "id": "1HL0pgcRuo7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Birden çok sözcükten oluşan yapılardır.\n",
        "#TBMM gibi(bu tarz sözcükleri ayırmasın tek olarak alsın istediğimiz zaman kullanırız.)"
      ],
      "metadata": {
        "id": "Ba539N2XurRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#n-gram --> n herhangi bir sayıdır.2-gram,3-gram gibi,bigram,trigram...\n",
        "#2-gram --> 2 li sözcükleri birlikte alıyor.Soldan sağa şekilde kaydırarak götürüyor.\n",
        "#Sözcükleri belirli bir pencere aralığı belirleyerek içerisinde almış olduğumuz birliktelirklerinden bahsedilir.(windowing)\n"
      ],
      "metadata": {
        "id": "mF86vswjvGLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "#ngram çıkarabilmemiz için öncelikle tokenizetion yapmamız gerekiyor.\n",
        "#Karakter bazlı da ngram çıkarılabiliyor\n",
        "\n",
        "sozcukler = word_tokenize(\"Bugün sabah okula arabayla gittim.\")\n",
        "\n",
        "ngramlar = list(ngrams(sozcukler,2))# bir object olduğu için doğrudan görme şansımız yok bu yüzden liste içerisine atıyoruz.\n",
        "print(sozcukler)\n",
        "print(ngramlar)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeO5_5h_wQJV",
        "outputId": "f2e0b6ca-7bc0-4718-99d0-6b5c9bcf3b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bugün', 'sabah', 'okula', 'arabayla', 'gittim', '.']\n",
            "[('Bugün', 'sabah'), ('sabah', 'okula'), ('okula', 'arabayla'), ('arabayla', 'gittim'), ('gittim', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ön işlemenin bir sonraki adımı verinin temzilenmesi işlemidir.(Büyük harflerin küçük harflere dönüştürülmesi gibi,noktalama işaretlerinin ayıklanması gibi,yazım yanlışlarının düzeltilmesi gibi)\n"
      ],
      "metadata": {
        "id": "RRxCj8diyCqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str5 = \"Bugün sabah okula arabayla gittim.\"\n",
        "\n",
        "print(str5.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PPTeZ05yutG",
        "outputId": "4f8e0c9c-80d8-486e-a1f2-0d297e60b8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bugün sabah okula arabayla gittim.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular Expression***"
      ],
      "metadata": {
        "id": "32NIw3-Ny8VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "str6 = \"Bugün sabah   okula saat      12 gibi arabayla gittim.\"\n",
        "\n",
        "str7 = re.sub(\"\\s+\",\" \",str6) #metnin içerisinde birden çok boşluk olan yerleri tek boşluğa dönüştürme.\n",
        "str8 = re.sub(\"\\d+\",\" \",str7) #içerisinde sayı olan değeri temzile ve boşlukla değiştir\n",
        "str9 = re.sub(\"\\.\",\" \",str8) #noktayı siler.\n",
        "# bir şeyi başka bir şeyle değiştirme işlemi yapar.(substitute)\n",
        "print(str7)\n",
        "print(str8)\n",
        "print(str9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN9C9-0Ly6Fd",
        "outputId": "398af72f-38ba-4943-a495-5c0cbe3c913c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bugün sabah okula saat 12 gibi arabayla gittim.\n",
            "Bugün sabah okula saat   gibi arabayla gittim.\n",
            "Bugün sabah okula saat   gibi arabayla gittim \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Word**"
      ],
      "metadata": {
        "id": "Ui1anlVx63iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bazı sözcüklerin atılması işlemidir.\n",
        "#TNC - Turkish National Corpus\n",
        "#Derlemler dilin içsel yapısının algoritmalar tarafından öğrenilebilmesi için bizlere olanak sağlıyor.\n",
        "#Derlemler dilin yapısına ilişkin ipuçları sunar.\n",
        "#functianol sözcüklere karşılık gelirler\n",
        "#Bazen bu stop wordlerin çıkarılması gerekir. Böylelikle modelin daha işlevsel ve etkin olmasını sağlar.\n",
        "\n"
      ],
      "metadata": {
        "id": "nZR-swv866Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Frequency**"
      ],
      "metadata": {
        "id": "og6sPpaz7r5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sözcük sıklığıdır.\n",
        "#Bir derlemi kullanarak dilin içerisindeki sözcüklerin geçme sıklıklarına bakabiliriz.\n",
        "#substantive sözcük--> dil bilim sözlüğünü açtığımızda orada görebileceğimiz sözcüklerdir.Bazen değişebilir.\n",
        "#functional sözcükler(işlevsel) --> ve, veya,bu,şu,ben... en sık geçen sözcüklerdir.Sözlükte bulunmazlar.Bir dil için sınırlı sayıdadır."
      ],
      "metadata": {
        "id": "AAc-K9zj7vgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpPlo_uR9o9K",
        "outputId": "fcf3ad9c-e602-4270-9aa3-8fe826b1e938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words(\"english\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teedGT3Q9Vno",
        "outputId": "408af429-cbea-48d6-b064-91ae048f88f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "fG-HoQCY-GnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Köke ulaşmak demektir.\n",
        "#PorterStemmer kütüphanesi daha çok ingilizce diline uygun bir kütüphane\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sozcukler = [\"cats\",\"cat\",\"child\",\"children\"]\n",
        "\n",
        "st_sozcukler = [stemmer.stem(sozcuk) for sozcuk in sozcukler]\n",
        "print(st_sozcukler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tec2qHey-LND",
        "outputId": "58fd6529-9cb2-48d9-9a15-0f276c495efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'cat', 'child', 'children']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "BtoikI98_G9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bir sözcüğün sözlükte bulunan en temel haline karşı gelir.\n",
        "#Temelde ulaşmaya çalıştığımız şey sözlükteki halidir.\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApyO4HEZ_LEY",
        "outputId": "87de4023-fd1f-4f0e-841f-f5e41f467591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lm_sozcukler = [lemmatizer.lemmatize(sozcuk) for sozcuk in sozcukler]\n",
        "\n",
        "print(lm_sozcukler)\n",
        "\n",
        "#children ın child kelimesin çoğul hali olduğunu bildiğinden kökünü child olarak yazdırır.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBn_zv9lARiO",
        "outputId": "da9a21d5-c048-4756-dbba-9eac12a749eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'cat', 'child', 'child']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhALJXqM_eXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Word Expression**"
      ],
      "metadata": {
        "id": "ONL19rdaxwbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Çok sözcüklü ifadelerin çıkartılması işlemleri."
      ],
      "metadata": {
        "id": "ry-BtHuox3df"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4weagfBqi6P1FxXPI3nTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}